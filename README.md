# ğŸ‡§ğŸ‡· README â€“ PortuguÃªs (Brasil)

## ğŸ§  NavegaÃ§Ã£o em Labirinto com Q-Learning
Este projeto implementa um agente inteligente capaz de navegar em um labirinto 2D utilizando o algoritmo de Q-Learning. O ambiente inclui paredes fixas, NPCs mÃ³veis que se deslocam de forma aleatÃ³ria e portas que alternam entre abertas e fechadas conforme o tempo, criando um cenÃ¡rio dinÃ¢mico e desafiador.

## ğŸ¯ Objetivos do Projeto
- Criar um ambiente de labirinto com elementos estÃ¡ticos e dinÃ¢micos.
- Implementar Q-Learning para treinar um agente a encontrar a saÃ­da.
- Avaliar o desempenho do agente em cenÃ¡rios estÃ¡tico, com NPCs e com portas dinÃ¢micas.
- Comparar as estratÃ©gias aprendidas.
- Identificar limitaÃ§Ãµes e sugerir melhorias futuras.

## ğŸ§© Funcionalidades
- Grid 10x14 representado em matriz.
- Portas com ciclos de abertura/fechamento.
- NPCs mÃ³veis.
- PolÃ­tica Îµ-greedy com decaimento.
- GeraÃ§Ã£o automÃ¡tica de grÃ¡ficos.
- Armazenamento dos resultados.

## ğŸ”§ Como Executar
pip install numpy matplotlib
python main.py

## ğŸ“Š Resultados
O sistema gera grÃ¡ficos como:
- Recompensa mÃ©dia por episÃ³dio  
- ComparaÃ§Ã£o entre cenÃ¡rios  
- ConvergÃªncia do aprendizado  

## ğŸ§­ Melhorias Futuras
- Testar Deep Q-Learning  
- Aumentar o tamanho do ambiente  
- NPCs com comportamento mais complexo  
- IntroduÃ§Ã£o de visÃ£o parcial  

## ğŸ“š ReferÃªncias
- Sutton & Barto (2018) â€“ Reinforcement Learning: An Introduction  
- Watkins (1989) â€“ Learning from Delayed Rewards  

---

# ğŸ‡ºğŸ‡¸ README â€“ English Version

## ğŸ§  Maze Navigation with Q-Learning
This project implements an intelligent agent capable of navigating a 2D maze using the Q-Learning algorithm. The environment includes fixed walls, randomly moving NPCs, and dynamic doors that open and close over time.

## ğŸ¯ Project Goals
- Build a maze with static and dynamic elements.
- Train an agent using Q-Learning to reach the exit efficiently.
- Evaluate the agent under static, NPC-based, and dynamic-door scenarios.
- Compare learned strategies.
- Identify limitations and propose improvements.

## ğŸ§© Features
- 10x14 matrix-based grid  
- Dynamic doors with time cycles  
- Random NPC movement  
- Îµ-greedy policy with decay  
- Automatic performance graphs  
- Result logging and storage  

## ğŸ”§ How to Run
pip install numpy matplotlib
python main.py

## ğŸ“Š Results
Generated figures include:
- Average reward per episode  
- Scenario comparison  
- Learning convergence  

## ğŸ§­ Future Work
- Apply Deep Q-Learning  
- Expand maze dimensions  
- More advanced NPC behaviors  
- Add partial observability  

## ğŸ“š References
- Sutton & Barto (2018) â€“ Reinforcement Learning: An Introduction  
- Watkins (1989) â€“ Learning from Delayed Rewards  
